import os
import time
from bs4 import BeautifulSoup
from selenium.common.exceptions import TimeoutException, StaleElementReferenceException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from modules.data_utils import get_driver

# âœ… TCMSP ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
TCMSP_DIR = "data/tcmsp"
os.makedirs(TCMSP_DIR, exist_ok=True)

class TCMSPScraper:
    BASE_URL = "https://tcmsp-e.com/tcmspsearch.php"
    MOLECULE_URL = "https://tcmsp-e.com/{path}"

    def __init__(self, herb_name, herb_url):
        self.herb_name = herb_name
        self.herb_url = herb_url
        self.driver = get_driver()
        self.driver.set_page_load_timeout(60)  # âœ… í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ 60ì´ˆ ì„¤ì •

    def get_total_pages(self):
        """ì´ í˜ì´ì§€ ìˆ˜ ê³„ì‚° (ìˆ«ì í˜ì´ì§€ ë§í¬ë§Œ ì¶”ì¶œ)"""
        pagination_links = self.driver.find_elements(By.CSS_SELECTOR, ".k-pager-numbers .k-link")
        
        # âœ… "ë‹¤ìŒ", "ì´ì „" ê°™ì€ ë¹„ìˆ«ì ì œê±°
        page_numbers = [
            int(link.get_attribute("data-page"))
            for link in pagination_links
            if link.get_attribute("data-page") and link.get_attribute("data-page").isdigit()
        ]

        total_pages = max(page_numbers) if page_numbers else 1
        print(f"âœ… ì´ í˜ì´ì§€ ìˆ˜: {total_pages}")
        return total_pages
    
    def get_ingredients(self):
        """ëª¨ë“  ì„±ë¶„ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ì¤‘ë³µ mol_id ì œì™¸ ì²˜ë¦¬)"""
        ingredients = []
        collected_mol_ids = set()  # âœ… ìˆ˜ì§‘ëœ mol_id ì¶”ì 

        # âœ… URL ë¡œë“œ (1í˜ì´ì§€)
        self.driver.get(self.herb_url)
        time.sleep(2)

        total_pages = self.get_total_pages()
        print(f"ğŸ“„ ì´ {total_pages} í˜ì´ì§€ ì˜ˆìƒ - ì„±ë¶„ í¬ë¡¤ë§ ì‹œì‘...")

        for page in range(1, total_pages + 1):
            try:
                print(f"â¡ï¸ {self.herb_name} - í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")

                # âœ… ì²« í˜ì´ì§€ ì œì™¸ ì‹œ í´ë¦­
                if page > 1:
                    pagination_links = self.driver.find_elements(By.CSS_SELECTOR, ".k-pager-numbers .k-link")
                    page_link = next(
                        (link for link in pagination_links
                        if link.get_attribute("data-page") == str(page)
                        and link.get_attribute("data-page").isdigit()), None)

                    if page_link:
                        self.driver.execute_script("arguments[0].click();", page_link)
                        WebDriverWait(self.driver, 10).until(EC.staleness_of(page_link))
                        time.sleep(2)

                soup = BeautifulSoup(self.driver.page_source, "html.parser")
                table = soup.select_one("#grid table tbody")

                if not table:
                    print(f"âš ï¸ í˜ì´ì§€ {page}ì— í…Œì´ë¸” ì—†ìŒ. í¬ë¡¤ë§ ì¤‘ë‹¨.")
                    break

                rows = table.find_all("tr")
                new_ingredients = []

                for row in rows:
                    cols = row.find_all("td")
                    if len(cols) < 10:
                        continue

                    mol_id = cols[0].text.strip()

                    # âœ… ì´ë¯¸ ìˆ˜ì§‘ëœ mol_idëŠ” ê±´ë„ˆëœ€
                    if mol_id in collected_mol_ids:
                        print(f"ğŸ” ì¤‘ë³µ mol_id ë°œê²¬: {mol_id} â†’ í•´ë‹¹ ì„±ë¶„ ê±´ë„ˆëœ€")
                        continue

                    mol_name_tag = cols[1].find("a")
                    mol_name = mol_name_tag.text.strip() if mol_name_tag else ""
                    mol_url = self.MOLECULE_URL.format(path=mol_name_tag["href"]) if mol_name_tag else ""
                    mw = cols[2].text.strip()
                    alogp = cols[3].text.strip()
                    hdon = cols[4].text.strip()
                    hacc = cols[5].text.strip()
                    ob = float(cols[6].text.strip()) if cols[6].text.strip() else 0
                    caco2 = cols[7].text.strip()
                    bbb = cols[8].text.strip()
                    dl = float(cols[9].text.strip()) if cols[9].text.strip() else 0
                    fasa = cols[10].text.strip()
                    halflife = cols[11].text.strip()

                    ingredient = {
                        "mol_id": mol_id,
                        "mol_name": mol_name,
                        "mol_url": mol_url,
                        "mw": mw,
                        "alogp": alogp,
                        "hdon": hdon,
                        "hacc": hacc,
                        "ob": ob,
                        "caco2": caco2,
                        "bbb": bbb,
                        "dl": dl,
                        "fasa": fasa,
                        "halflife": halflife
                    }

                    new_ingredients.append(ingredient)
                    collected_mol_ids.add(mol_id)  # âœ… mol_id ë“±ë¡

                ingredients.extend(new_ingredients)

                # âœ… ì´ë²ˆ í˜ì´ì§€ì—ì„œ ìƒˆ ì„±ë¶„ì„ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìœ¼ë©´ ì¤‘ë‹¨
                if not new_ingredients:
                    print(f"âš ï¸ í˜ì´ì§€ {page}ì—ì„œ ìƒˆë¡œìš´ ì„±ë¶„ ì—†ìŒ â†’ í¬ë¡¤ë§ ì¤‘ë‹¨")
                    break

            except Exception as e:
                print(f"âŒ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue

        print(f"âœ… {self.herb_name} ì„±ë¶„ í¬ë¡¤ë§ ì™„ë£Œ. ì´ {len(ingredients)}ê°œ ì„±ë¶„ ìˆ˜ì§‘ë¨.")
        return ingredients
    
    def get_targets(self, ingredients, max_retries=5):
        """ì„±ë¶„ë³„ íƒ€ê²Ÿ í¬ë¡¤ë§ (ë¡œë”© ì•ˆì •ì„± ê°œì„  ë° StaleElementReference í•´ê²°)"""
        targets = []

        for idx, ingredient in enumerate(ingredients, 1):
            mol_url = ingredient.get("mol_url")
            if not mol_url:
                print(f"âš ï¸ {ingredient['mol_name']}ì— mol_url ì—†ìŒ - ê±´ë„ˆëœ€")
                continue

            print(f"ğŸ” [{idx}/{len(ingredients)}] {ingredient['mol_name']} íƒ€ê²Ÿ í¬ë¡¤ë§ ì¤‘...")

            retries = 0
            while retries < max_retries:
                try:
                    # âœ… í˜ì´ì§€ ë¡œë“œ ë° ë¡œë”© ëŒ€ê¸°
                    self.driver.get(mol_url)
                    WebDriverWait(self.driver, 15).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                    time.sleep(1)  # ì ê¹ì˜ ëŒ€ê¸°

                    # âœ… ì´ˆê¸° í˜ì´ì§€ ìˆ˜ í™•ì¸
                    pagination_links = self.driver.find_elements(By.CSS_SELECTOR, "#kendo_target .k-pager-numbers .k-link")
                    total_pages = max(
                        [int(link.get_attribute("data-page")) for link in pagination_links if link.get_attribute("data-page")],
                        default=1
                    )

                    page = 1
                    visited_pages = set()

                    while True:
                        print(f"ğŸ“‚ íƒ€ê²Ÿ í˜ì´ì§€ {page}/{total_pages}")

                        # âœ… í…Œì´ë¸” ë¡œë”© ëŒ€ê¸° (ìµœëŒ€ 10ì´ˆ)
                        try:
                            WebDriverWait(self.driver, 10).until(
                                EC.presence_of_element_located((By.CSS_SELECTOR, "#kendo_target table tbody"))
                            )
                        except TimeoutException:
                            print("âš ï¸ í…Œì´ë¸” ë¡œë”© ì‹¤íŒ¨ - N/A ê°’ìœ¼ë¡œ ëŒ€ì²´")
                            targets.append({
                                "mol_id": ingredient["mol_id"],
                                "mol_name": ingredient["mol_name"],
                                "target_name": "N/A",
                                "target_id": "N/A",
                                "drugbank_id": "N/A",
                                **{key: ingredient[key] for key in ["mw", "alogp", "hdon", "hacc", "ob", "caco2", "bbb", "dl", "fasa", "halflife"]}
                            })
                            break

                        # âœ… íƒ€ê²Ÿ ì •ë³´ ìˆ˜ì§‘
                        soup = BeautifulSoup(self.driver.page_source, "html.parser")
                        table = soup.select_one("#kendo_target table tbody")

                        if table:
                            for row in table.find_all("tr"):
                                cols = row.find_all("td")
                                if len(cols) < 2:
                                    continue

                                targets.append({
                                    "mol_id": ingredient["mol_id"],
                                    "mol_name": ingredient["mol_name"],
                                    "target_name": cols[0].text.strip() or "N/A",
                                    "target_id": cols[0].find("a")["href"].split("=")[-1] if cols[0].find("a") else "N/A",
                                    "drugbank_id": cols[1].find("a")["href"].split("/")[-1] if cols[1].find("a") else "N/A",
                                    **{key: ingredient[key] for key in ["mw", "alogp", "hdon", "hacc", "ob", "caco2", "bbb", "dl", "fasa", "halflife"]}
                                })

                        visited_pages.add(page)

                        # âœ… ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬ ì‹œ ì¢…ë£Œ
                        if page >= total_pages:
                            print("âœ… ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬ - íƒ€ê²Ÿ í¬ë¡¤ë§ ì¢…ë£Œ")
                            break

                        # âœ… 11í˜ì´ì§€ ë„ë‹¬ ì‹œ í˜ì´ì§€ ìˆ˜ ì¬í™•ì¸
                        if page == 11:
                            pagination_links = self.driver.find_elements(By.CSS_SELECTOR, "#kendo_target .k-pager-numbers .k-link")
                            updated_total_pages = max(
                                [int(link.get_attribute("data-page")) for link in pagination_links if link.get_attribute("data-page")],
                                default=total_pages
                            )
                            if updated_total_pages > total_pages:
                                print(f"ğŸ”„ í˜ì´ì§€ ìˆ˜ ì¬í™•ì¸: {total_pages} â†’ {updated_total_pages}")
                                total_pages = updated_total_pages

                        # âœ… ë‹¤ìŒ í˜ì´ì§€ í´ë¦­
                        pagination_links = self.driver.find_elements(By.CSS_SELECTOR, "#kendo_target .k-pager-numbers .k-link")
                        next_page_link = next(
                            (link for link in pagination_links if int(link.get_attribute("data-page")) == page + 1),
                            None
                        )

                        if not next_page_link:
                            print("âœ… ë” ì´ìƒ í˜ì´ì§€ ì—†ìŒ - íƒ€ê²Ÿ í¬ë¡¤ë§ ì¢…ë£Œ")
                            break

                        # âœ… í´ë¦­ ì‹œ ìƒˆ ìš”ì†Œë¡œ ëŒ€ì²´ ë° ëŒ€ê¸°
                        self.driver.execute_script("arguments[0].click();", next_page_link)
                        WebDriverWait(self.driver, 10).until(
                            EC.staleness_of(next_page_link)
                        )
                        page += 1
                        time.sleep(1)  # í´ë¦­ í›„ ì ì‹œ ëŒ€ê¸°

                    break  # âœ… í¬ë¡¤ë§ ì„±ê³µ ì‹œ ì¬ì‹œë„ ì¤‘ë‹¨

                except Exception as e:
                    retries += 1
                    print(f"âš ï¸ {mol_url} í¬ë¡¤ë§ ì‹¤íŒ¨ (ì‹œë„ {retries}/{max_retries}): {e}")
                    if retries == max_retries:
                        print(f"âŒ {mol_url} ìµœëŒ€ ì¬ì‹œë„ ì‹¤íŒ¨ - ê±´ë„ˆëœ€")

        print(f"âœ… íƒ€ê²Ÿ í¬ë¡¤ë§ ì™„ë£Œ. ì´ {len(targets)}ê°œ íƒ€ê²Ÿ ìˆ˜ì§‘ë¨.")
        return targets


    def scrape(self):
        """ì „ì²´ í¬ë¡¤ë§ (ì„±ë¶„ + íƒ€ê²Ÿ)"""
        print(f"ğŸš€ {self.herb_name} í¬ë¡¤ë§ ì‹œì‘...")
        ingredients = self.get_ingredients()
        targets = self.get_targets(ingredients)
        return {"herb": self.herb_name, "ingredients": ingredients, "targets": targets}
